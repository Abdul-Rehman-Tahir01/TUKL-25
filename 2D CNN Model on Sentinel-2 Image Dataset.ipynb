{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-17T14:27:13.137723Z",
     "iopub.status.busy": "2025-07-17T14:27:13.137155Z",
     "iopub.status.idle": "2025-07-17T14:27:19.346356Z",
     "shell.execute_reply": "2025-07-17T14:27:19.345389Z",
     "shell.execute_reply.started": "2025-07-17T14:27:13.137693Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.2/22.2 MB\u001b[0m \u001b[31m68.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q rasterio  # -q is for quiet installation, non-verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-07-17T14:27:22.562248Z",
     "iopub.status.busy": "2025-07-17T14:27:22.561925Z",
     "iopub.status.idle": "2025-07-17T14:27:28.919055Z",
     "shell.execute_reply": "2025-07-17T14:27:28.918369Z",
     "shell.execute_reply.started": "2025-07-17T14:27:22.562220Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import rasterio\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-17T14:27:37.746036Z",
     "iopub.status.busy": "2025-07-17T14:27:37.745569Z",
     "iopub.status.idle": "2025-07-17T14:27:37.843315Z",
     "shell.execute_reply": "2025-07-17T14:27:37.842517Z",
     "shell.execute_reply.started": "2025-07-17T14:27:37.746013Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Paths and settings\n",
    "BASE_PATH      = '/kaggle/input/sentinel2-crop-mapping'\n",
    "H5_PATH        = '/kaggle/working/tiles_2016-18_uint8.h5'\n",
    "REGIONS        = ['lombardia', 'lombardia2']\n",
    "YEARS          = ['data2016', 'data2017', 'data2018']\n",
    "NUM_TIMESTEPS  = 32\n",
    "BATCH_SIZE     = 64\n",
    "NUM_WORKERS    = 4\n",
    "PIN_MEMORY     = True\n",
    "LR             = 0.01\n",
    "NUM_EPOCHS     = 10\n",
    "DEVICE         = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preprocessing and Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-17T14:27:59.956486Z",
     "iopub.status.busy": "2025-07-17T14:27:59.956140Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating HDF5 archive at /kaggle/working/tiles_2016-18_uint8.h5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing HDF5:  16%|█▋        | 3281/19969 [20:05<1:35:49,  2.90it/s]"
     ]
    }
   ],
   "source": [
    "# 1) Preprocess: pack all tiles into one HDF5\n",
    "if not os.path.exists(H5_PATH):\n",
    "    print(f\"Creating HDF5 archive at {H5_PATH}...\")\n",
    "\n",
    "    # gather tile directories\n",
    "    tile_dirs = []\n",
    "    for reg in REGIONS:\n",
    "        for yr in YEARS:\n",
    "            tile_dirs += glob.glob(os.path.join(BASE_PATH, reg, yr, '*'))\n",
    "    N = len(tile_dirs)\n",
    "\n",
    "    with h5py.File(H5_PATH, 'w') as hf:\n",
    "        imgs = hf.create_dataset(\n",
    "            'images',\n",
    "            shape=(N, 9*NUM_TIMESTEPS, 48, 48),\n",
    "            dtype='uint8',\n",
    "            chunks=(1, 9*NUM_TIMESTEPS, 48, 48),\n",
    "            compression='lzf')\n",
    "        masks = hf.create_dataset(\n",
    "            'masks',\n",
    "            shape=(N, 48, 48),\n",
    "            dtype='uint8',\n",
    "            chunks=(1, 48, 48),\n",
    "            compression='lzf')\n",
    "\n",
    "        write_idx = 0\n",
    "        for td in tqdm(tile_dirs, desc='Writing HDF5'):\n",
    "            if not os.path.isdir(td):\n",
    "                continue\n",
    "\n",
    "            # list & sort your 32 multispectral .tifs\n",
    "            tifs = sorted([\n",
    "                f for f in os.listdir(td)\n",
    "                if f.endswith('.tif') and '_MSAVI' not in f and f != 'y.tif'\n",
    "            ])[:NUM_TIMESTEPS]\n",
    "\n",
    "            # skip if too few files\n",
    "            if len(tifs) < NUM_TIMESTEPS:\n",
    "                continue\n",
    "\n",
    "            # load & stack\n",
    "            stack = []\n",
    "            for f in tifs:\n",
    "                with rasterio.open(os.path.join(td, f)) as src:\n",
    "                    stack.append(src.read().astype(np.float32))\n",
    "            img = np.stack(stack, axis=1).reshape(-1, 48, 48)\n",
    "\n",
    "            # quantize\n",
    "            img_min, img_max = img.min(), img.max()\n",
    "            img_q = ((img - img_min) / (img_max - img_min) * 255.0)\\\n",
    "                      .round().astype(np.uint8)\n",
    "\n",
    "            # load mask\n",
    "            with rasterio.open(os.path.join(td, 'y.tif')) as src:\n",
    "                mask = src.read(1).astype(np.uint8)\n",
    "            mask[mask >= 20] = 255\n",
    "\n",
    "            # write into HDF5\n",
    "            imgs[write_idx]  = img_q\n",
    "            masks[write_idx] = mask\n",
    "            write_idx += 1\n",
    "\n",
    "        # after the loop, resize to actual count\n",
    "        hf['images'].resize(write_idx, axis=0)\n",
    "        hf['masks'].resize(write_idx, axis=0)\n",
    "\n",
    "    print(\"HDF5 archive created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 2) Dataset: lazy-load from HDF5 and rescale\n",
    "class HDF5SentinelDataset(Dataset):\n",
    "    def __init__(self, h5_path, indices, transform=None):\n",
    "        self.hf      = h5py.File(h5_path, 'r')\n",
    "        self.imgs    = self.hf['images']\n",
    "        self.masks   = self.hf['masks']\n",
    "        self.indices = indices\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        i   = self.indices[idx]\n",
    "        img = torch.from_numpy(self.imgs[i]).float() / 255.0\n",
    "        msk = torch.from_numpy(self.masks[i]).long()\n",
    "        if self.transform:\n",
    "            img, msk = self.transform(img, msk)\n",
    "        return img, msk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 3) Prepare train/val splits and dataloaders\n",
    "with h5py.File(H5_PATH, 'r') as hf:\n",
    "    total = hf['images'].shape[0]\n",
    "all_indices = list(range(total))\n",
    "train_idx, val_idx = train_test_split(all_indices, test_size=0.2, random_state=42)\n",
    "\n",
    "train_ds = HDF5SentinelDataset(H5_PATH, train_idx)\n",
    "val_ds   = HDF5SentinelDataset(H5_PATH, val_idx)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> Shape of the input is (9*32, 48, 48) and of output is (48, 48) with batch size of 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Visualizing the images (rgb composite for multispectral input)\n",
    "def normalize_band(band):\n",
    "    \"\"\"Contrast stretch to 2–98 percentile\"\"\"\n",
    "    p2, p98 = np.percentile(band, (2, 98))\n",
    "    return np.clip((band - p2) / (p98 - p2), 0, 1)\n",
    "    \n",
    "\n",
    "# Print from train dataset\n",
    "x_train, y_train = train_dataset[0]\n",
    "print(\"Train Sample:\")\n",
    "print(f\"x_train shape: {x_train.shape}\")  # Expected: (288, 48, 48)\n",
    "print(f\"y_train shape: {y_train.shape}\")  # Expected: (48, 48)\")\n",
    "print(f\"x_train dtype: {x_train.dtype}\")\n",
    "print(f\"y_train unique labels: {torch.unique(y_train)}\")  # Sanity check\n",
    "\n",
    "# Extract bands 4, 3, 2 from timestep 1\n",
    "b2 = 1   # Blue\n",
    "b3 = 2   # Green\n",
    "b4 = 3   # Red\n",
    "\n",
    "rgb = x_train[[b4, b3, b2]]  # Shape: (3, 48, 48)\n",
    "\n",
    "r = normalize_band(rgb[0].numpy())\n",
    "g = normalize_band(rgb[1].numpy())\n",
    "b = normalize_band(rgb[2].numpy())\n",
    "\n",
    "rgb_normalized = np.stack([r, g, b], axis=-1)\n",
    "\n",
    "# Show corrected image\n",
    "plt.imshow(rgb_normalized)\n",
    "plt.title(\"RGB Composite (Timestep 1, Normalized) - Train Sample\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "# Show mask\n",
    "plt.imshow(y_train, cmap='tab20')\n",
    "plt.title(\"Ground Truth Mask - Train Sample\")\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Print from val dataset\n",
    "x_val, y_val = val_dataset[0]\n",
    "print(\"\\nValidation Sample:\")\n",
    "print(f\"x_val shape: {x_val.shape}\")  # Expected: (288, 48, 48)\n",
    "print(f\"y_val shape: {y_val.shape}\")  # Expected: (48, 48)\")\n",
    "print(f\"x_val dtype: {x_val.dtype}\")\n",
    "print(f\"y_val unique labels: {torch.unique(y_val)}\")\n",
    "\n",
    "\n",
    "# For Val data\n",
    "rgb = x_val[[b4, b3, b2]]  # Shape: (3, 48, 48)\n",
    "\n",
    "r = normalize_band(rgb[0].numpy())\n",
    "g = normalize_band(rgb[1].numpy())\n",
    "b = normalize_band(rgb[2].numpy())\n",
    "\n",
    "rgb_normalized = np.stack([r, g, b], axis=-1)\n",
    "\n",
    "# Show corrected image\n",
    "plt.imshow(rgb_normalized)\n",
    "plt.title(\"RGB Composite (Timestep 1, Normalized) - Val Sample\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "# Show mask\n",
    "plt.imshow(y_val, cmap='tab20')\n",
    "plt.title(\"Ground Truth Mask - Val Sample\")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D-CNN Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "in_channels = 9 * NUM_TIMESTEPS\n",
    "num_classes = 20\n",
    "\n",
    "lenet_5 = torch.nn.Sequential(\n",
    "    torch.nn.Conv2d(in_channels, 6, kernel_size=5, padding=2), # C1: Conv (5x5), 6 filters:  (288 → 6), output: (6, 44, 44)\n",
    "    torch.nn.Tanh(),\n",
    "    torch.nn.AvgPool2d(kernel_size=2, stride=2),    # S1: Avg Pooling (2x2): output: (6, 22, 22)\n",
    "\n",
    "    torch.nn.Conv2d(6, 16, kernel_size=5, padding=2),          # C2: Conv (5x5), 16 filters: output: (16, 18, 18)\n",
    "    torch.nn.Tanh(),\n",
    "    torch.nn.AvgPool2d(kernel_size=2, stride=2),    # S2: Avg Pooling (2x2): output: (16, 9, 9)\n",
    "\n",
    "    torch.nn.Conv2d(16, num_classes, kernel_size=1),        # output: (20, 9, 9) — segmentation logits\n",
    "    torch.nn.Upsample(size=(48, 48), mode='bilinear', align_corners=False)  # Upsample to (48, 48)\n",
    ").to(DEVICE)\n",
    "\n",
    "print(lenet_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> The FC layers are removed and instead a Conv2D layer is used since we want the 2D segmentation map and FC layers flatten the 2d in 1d."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=255)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def pixel_accuracy(out, target):\n",
    "    \"\"\"\n",
    "    Calculating pixel-wise accuracy by comparing the predicted class per pixel with the \n",
    "    ground truth and compute how many pixels were correctly classified.\n",
    "    \n",
    "    pred: tensor of shape (B, C, H, W) - raw logits\n",
    "    target: tensor of shape (B, H, W) - class labels \n",
    "    \"\"\"\n",
    "    preds = out.argmax(dim=1)\n",
    "    return (preds == target).float().mean().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 5) Training loop\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    model.train()\n",
    "    t_loss, t_acc = 0.0, 0.0\n",
    "    for x, y in tqdm(train_loader, desc=f'Epoch {epoch} Training'):\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x)\n",
    "        loss = criterion(out, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        t_loss += loss.item()\n",
    "        t_acc  += pixel_accuracy(out, y)\n",
    "    t_loss /= len(train_loader)\n",
    "    t_acc  /= len(train_loader)\n",
    "\n",
    "    train_losses.append(t_loss)\n",
    "    train_accuracies.append(t_acc)\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    v_loss, v_acc = 0.0, 0.0\n",
    "    for x, y in tqdm(val_loader, desc=f'Epoch {epoch} Validating'):\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "        out = model(x)\n",
    "        loss = criterion(out, y)\n",
    "        v_loss += loss.item()\n",
    "        v_acc  += pixel_accuracy(out, y)\n",
    "    v_loss /= len(val_loader)\n",
    "    v_acc  /= len(val_loader)\n",
    "\n",
    "    val_losses.append(v_loss)\n",
    "    val_accuracies.append(v_acc)\n",
    "\n",
    "    print(f\"Epoch {epoch}/{NUM_EPOCHS} | \"\n",
    "          f\"Train Loss: {t_loss:.4f}, Acc: {t_acc:.4f} | \"\n",
    "          f\"Val Loss: {v_loss:.4f}, Acc: {v_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "epochs = range(1, NUM_EPOCHS + 1)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# ---- LOSS ----\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, train_losses, label='Train Loss', color='blue', linewidth=2)\n",
    "plt.plot(epochs, val_losses,   label='Val Loss',   color='orange', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss vs Epoch')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# ---- ACCURACY ----\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, train_accuracies, label='Train Acc', color='green', linewidth=2)\n",
    "plt.plot(epochs, val_accuracies,   label='Val Acc',   color='red', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Pixel Accuracy')\n",
    "plt.title('Accuracy vs Epoch')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def visualize_prediction(model, dataset, idx=0):\n",
    "    model.eval()\n",
    "    x, y = dataset[idx]\n",
    "    x = x.unsqueeze(0).to(device)  # Add batch dim\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred = model(x)  # (1, 20, 48, 48)\n",
    "        pred_mask = pred.argmax(dim=1).squeeze().cpu()  # (48, 48)\n",
    "\n",
    "    # Plot\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(8, 4))\n",
    "    axs[0].imshow(y, cmap='tab20')\n",
    "    axs[0].set_title(\"Ground Truth\")\n",
    "    axs[1].imshow(pred_mask, cmap='tab20')\n",
    "    axs[1].set_title(\"Predicted Mask\")\n",
    "    for ax in axs:\n",
    "        ax.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Try on train and val\n",
    "visualize_prediction(model, train_dataset, idx=0)\n",
    "visualize_prediction(model, val_dataset, idx=0)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 2134482,
     "sourceId": 3549868,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
